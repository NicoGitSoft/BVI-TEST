import numpy as np
import depthai as dai
from pathlib import Path
import time, json, cv2
from math import pi, floor, atan2, cos, sin
from scipy.special import expit

def to_planar(arr: np.ndarray, shape: tuple) -> np.ndarray:
    return cv2.resize(arr, shape).transpose(2,0,1)#.flatten()

def warp_rect_img(rect_points, img, w, h):
        src = np.array(rect_points[1:], dtype=np.float32) # rect_points[0] is left bottom point !
        dst = np.array([(0, 0), (h, 0), (h, w)], dtype=np.float32)
        mat = cv2.getAffineTransform(src, dst)
        return cv2.warpAffine(img, mat, (w, h))

def normalize_radians(angle):
    return angle - 2 * pi * floor((angle + pi) / (2 * pi))

def rotated_rect_to_points(cx, cy, w, h, rotation):
    b = cos(rotation) * 0.5
    a = sin(rotation) * 0.5
    p0x = cx - a*h - b*w
    p0y = cy + b*h - a*w
    p1x = cx + a*h - b*w
    p1y = cy - b*h - a*w
    p2x = int(2*cx - p0x)
    p2y = int(2*cy - p0y)
    p3x = int(2*cx - p1x)
    p3y = int(2*cy - p1y)
    p0x, p0y, p1x, p1y = int(p0x), int(p0y), int(p1x), int(p1y)
    return [[p0x,p0y], [p1x,p1y], [p2x,p2y], [p3x,p3y]]

def decode_bboxes(score_thresh, scores, bboxes, anchors, scale=128, best_only=False):
    regions = []
    scores = expit(scores) # 1 / (1 + np.exp(-scores))
    if best_only:
        best_id = np.argmax(scores)
        if scores[best_id] < score_thresh: return regions
        det_scores = scores[best_id:best_id+1]
        det_bboxes2 = bboxes[best_id:best_id+1]
        det_anchors = anchors[best_id:best_id+1]
    else:
        detection_mask = scores > score_thresh
        det_scores = scores[detection_mask]
        if det_scores.size == 0: return regions
        det_bboxes2 = bboxes[detection_mask]
        det_anchors = anchors[detection_mask]
    det_bboxes = det_bboxes2* np.tile(det_anchors[:,2:4], 9) / scale + np.tile(det_anchors[:,0:2],9)
    det_bboxes[:,2:4] = det_bboxes[:,2:4] - det_anchors[:,0:2]
    # box = [cx - w*0.5, cy - h*0.5, w, h]
    det_bboxes[:,0:2] = det_bboxes[:,0:2] - det_bboxes[:,3:4] * 0.5
    for i in range(det_bboxes.shape[0]):
        score = det_scores[i]
        box = det_bboxes[i,0:4]
        if box[2] < 0 or box[3] < 0: continue
        kps = []
        for kp in range(7):
            kps.append(det_bboxes[i,4+kp*2:6+kp*2])
        regions.append(HandRegion(float(score), box, kps))
    return regions

def detections_to_rect(regions):
    target_angle = pi * 0.5 # 90 = pi/2
    for region in regions:
        
        region.rect_w = region.pd_box[2]
        region.rect_h = region.pd_box[3]
        region.rect_x_center = region.pd_box[0] + region.rect_w / 2
        region.rect_y_center = region.pd_box[1] + region.rect_h / 2

        x0, y0 = region.pd_kps[0] # wrist center
        x1, y1 = region.pd_kps[2] # middle finger
        rotation = target_angle - atan2(-(y1 - y0), x1 - x0)
        region.rotation = normalize_radians(rotation)

def rect_transformation(regions, w, h):
    scale_x = 2.9
    scale_y = 2.9
    shift_x = 0
    shift_y = -0.5
    for region in regions:
        width = region.rect_w
        height = region.rect_h
        rotation = region.rotation
        if rotation == 0:
            region.rect_x_center_a = (region.rect_x_center + width * shift_x) * w
            region.rect_y_center_a = (region.rect_y_center + height * shift_y) * h
        else:
            x_shift = (w * width * shift_x * cos(rotation) - h * height * shift_y * sin(rotation)) #/ w
            y_shift = (w * width * shift_x * sin(rotation) + h * height * shift_y * cos(rotation)) #/ h
            region.rect_x_center_a = region.rect_x_center*w + x_shift
            region.rect_y_center_a = region.rect_y_center*h + y_shift

        # square_long: true
        long_side = max(width * w, height * h)
        region.rect_w_a = long_side * scale_x
        region.rect_h_a = long_side * scale_y
        region.rect_points = rotated_rect_to_points(region.rect_x_center_a, region.rect_y_center_a, region.rect_w_a, region.rect_h_a, region.rotation)


def hand_landmarks_to_rect(hand):
    # Calculates the ROI for the next frame from the current hand landmarks
    id_wrist = 0
    id_index_mcp = 5
    id_middle_mcp = 9
    id_ring_mcp =13
    
    lms_xy =  hand.landmarks[:,:2]
    # print(lms_xy)
    # Compute rotation
    x0, y0 = lms_xy[id_wrist]
    x1, y1 = 0.25 * (lms_xy[id_index_mcp] + lms_xy[id_ring_mcp]) + 0.5 * lms_xy[id_middle_mcp]
    rotation = 0.5 * pi - atan2(y0 - y1, x1 - x0)
    rotation = normalize_radians(rotation)
    # Now we work only on a subset of the landmarks
    ids_for_bounding_box = [0, 1, 2, 3, 5, 6, 9, 10, 13, 14, 17, 18]
    lms_xy = lms_xy[ids_for_bounding_box]
    # Find center of the boundaries of landmarks
    axis_aligned_center = 0.5 * (np.min(lms_xy, axis=0) + np.max(lms_xy, axis=0))
    # Find boundaries of rotated landmarks
    original = lms_xy - axis_aligned_center
    c, s = np.cos(rotation), np.sin(rotation)
    rot_mat = np.array(((c, -s), (s, c)))
    projected = original.dot(rot_mat)
    min_proj = np.min(projected, axis=0)
    max_proj = np.max(projected, axis=0)
    projected_center = 0.5 * (min_proj + max_proj)
    center = rot_mat.dot(projected_center) + axis_aligned_center
    width, height = max_proj - min_proj
    next_hand = HandRegion()
    next_hand.rect_w_a = next_hand.rect_h_a = 2 * max(width, height)
    next_hand.rect_x_center_a = center[0] + 0.1 * height * s
    next_hand.rect_y_center_a = center[1] - 0.1 * height * c
    next_hand.rotation = rotation
    next_hand.rect_points = rotated_rect_to_points(
        next_hand.rect_x_center_a,
        next_hand.rect_y_center_a,
        next_hand.rect_w_a,
        next_hand.rect_h_a,
        next_hand.rotation)
    return next_hand

class HandRegion:
    def __init__(self, pd_score=None, pd_box=None, pd_kps=None):
        self.pd_score = pd_score # Palm detection score 
        self.pd_box = pd_box # Palm detection box [x, y, w, h] normalized
        self.pd_kps = pd_kps # Palm detection keypoints

    def get_rotated_world_landmarks(self):
        world_landmarks_rotated = self.world_landmarks.copy()
        sin_rot = sin(self.rotation)
        cos_rot = cos(self.rotation)
        rot_m = np.array([[cos_rot, sin_rot], [-sin_rot, cos_rot]])
        world_landmarks_rotated[:,:2] = np.dot(world_landmarks_rotated[:,:2], rot_m)
        return world_landmarks_rotated


SCRIPT_DIR = Path(__file__).resolve().parent
PALM_DETECTION_MODEL = str(SCRIPT_DIR / "Models/Hands Models/palm_detection_sh4.blob")
LANDMARK_MODEL_LITE = str(SCRIPT_DIR / "Models/Hands Models/hand_landmark_lite_sh4.blob")
YOLO_MODEL = str(SCRIPT_DIR / "Models/BVI Models/best3_openvino_2021.4_6shave.blob")
YOLO_CONFIG = str(SCRIPT_DIR / "Models/BVI Models/best3.json")

class YoloHandTracker:
    def __init__(self,
        pd_model=PALM_DETECTION_MODEL,
        ml_model=LANDMARK_MODEL_LITE,
        yolo_model=YOLO_MODEL,
        yolo_configurations=YOLO_CONFIG,
        use_yolo=True,
        use_hand=True,
        temperature_sensing=False, 
        pd_score_thresh=0.5,
        pd_nms_thresh=0.3,
        use_lm=True,
        lm_score_thresh=0.5,
        use_world_landmarks=False,
        solo=True,
        crop=True,
        xyz=True,
        internal_fps=23,
        internal_frame_height=640,
        use_handedness_average=False,
        single_hand_tolerance_thresh=10,
        stats=False,
        trace=0, 
        ):

        self.temperature_sensing = temperature_sensing
        if temperature_sensing: print("Temperature Sensing Enabled")

        self.use_hand = use_hand
        if use_hand: print("Hand Tracking Enabled")

        self.use_yolo = use_yolo
        if use_yolo:
            print("Yolo Enabled")
            self.yolo_model = yolo_model
            self.yolo_configurations = yolo_configurations
            print(f"YoloModel blob          : {self.yolo_model}")
            print(f"YoloConfigurations json : {self.yolo_configurations}")

            # Extraer metadata del archivo de configuraciÃ³n .json
            with open(self.yolo_configurations, 'r') as file:
                config = json.load(file)
            metadata = config.get("nn_config").get("NN_specific_metadata")
            self.yolo_classes = metadata.get("classes")
            self.yolo_coordinates = metadata.get("coordinates")
            self.yolo_anchors = metadata.get("anchors")
            self.yolo_anchor_masks = metadata.get("anchor_masks")
            self.yolo_iou_threshold = metadata.get("iou_threshold")
            self.yolo_confidence_threshold = metadata.get("confidence_threshold")
            self.yolo_labels = config.get("mappings").get("labels")
            self.width, self.height = tuple(map(int, config.get("nn_config").get("input_size").split("x")))

        self.pd_model = pd_model
        print(f"Palm detection blob     : {self.pd_model}")
        self.lm_model = ml_model
        print(f"Landmark blob           : {self.lm_model}")
        self.pd_score_thresh = pd_score_thresh
        self.pd_nms_thresh = pd_nms_thresh
        self.use_lm = use_lm
        self.lm_score_thresh = lm_score_thresh
        self.solo = solo
        self.lm_nb_threads = 2      
        self.max_hands = 1
        self.xyz = xyz
        self.crop = crop 
        self.use_world_landmarks = use_world_landmarks
        self.internal_fps = internal_fps     
        self.stats = stats
        self.trace = trace
        self.use_handedness_average = use_handedness_average
        self.single_hand_tolerance_thresh = single_hand_tolerance_thresh

        # Create Device
        self.device = dai.Device()

        self.resolution = (1920, 1080)
        print("Sensor resolution:", self.resolution)
        self.frame_size, self.scale_nd = internal_frame_height, (16, 27) # mpu.find_isp_scale_params(internal_frame_height, self.resolution)
        self.img_h = self.img_w = self.frame_size
        self.pad_w = self.pad_h = 0
        self.crop_w = (int(round(self.resolution[0] * self.scale_nd[0] / self.scale_nd[1])) - self.img_w) // 2
        print(f"Internal camera image size: {self.img_w} x {self.img_h} - crop_w:{self.crop_w} pad_h: {self.pad_h}")
        
        # MediaPipe Anchors 
        self.pd_input_length = 128 # 192 Palm detection
        self.anchors = np.loadtxt( str(SCRIPT_DIR / "MediaPipeAnchors.txt"), dtype=np.float16) # Open anchors file
        self.nb_anchors = self.anchors.shape[0]

        # Define and start pipeline
        self.device.startPipeline(self.create_pipeline())

        # Define data queues
        self.q_video = self.device.getOutputQueue(name="cam_out", maxSize=1, blocking=False)
        self.q_stereo_out = self.device.getOutputQueue(name="stereo_out", maxSize=4, blocking=False)

        if self.use_hand:
            self.q_pd_out = self.device.getOutputQueue(name="pd_out", maxSize=2, blocking=False)
            self.q_manip_cfg = self.device.getInputQueue(name="manip_cfg")
            self.q_lm_out = self.device.getOutputQueue(name="lm_out", maxSize=2, blocking=False)
            self.q_lm_in = self.device.getInputQueue(name="lm_in")
        if self.use_yolo:
            self.q_yolo_out = self.device.getOutputQueue(name="yolo_out", maxSize=4, blocking=False)
        if self.temperature_sensing:
            self.qSysInfo = self.device.getOutputQueue(name="sysinfo", maxSize=4, blocking=False)

        self.nb_frames_pd_inference = 0
        self.nb_frames_lm_inference = 0
        self.nb_lm_inferences = 0
        self.nb_failed_lm_inferences = 0
        self.nb_frames_lm_inference_after_landmarks_ROI = 0
        self.nb_frames_no_hand = 0
        self.nb_spatial_requests = 0
        self.glob_pd_rtrip_time = 0
        self.glob_lm_rtrip_time = 0
        self.glob_spatial_rtrip_time = 0
        self.use_previous_landmarks = False
        self.nb_hands_in_previous_frame = 0
        

    def create_pipeline(self):
        print("Creating pipeline...")
        # Start defining a pipeline
        pipeline = dai.Pipeline()
        pipeline.setOpenVINOVersion(version = dai.OpenVINO.Version.VERSION_2021_4)

        # ColorCamera
        print("Creating Color Camera...")
        cam = pipeline.createColorCamera()
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam.setInterleaved(False)
        cam.setIspScale(self.scale_nd[0], self.scale_nd[1])
        cam.setFps(self.internal_fps)
        cam.setVideoSize(self.frame_size, self.frame_size)
        cam.setPreviewSize(self.img_w, self.img_h)
        cam_out = pipeline.createXLinkOut()
        cam_out.setStreamName("cam_out")
        cam_out.input.setQueueSize(1)
        cam_out.input.setBlocking(False)
        cam.video.link(cam_out.input)

        mono_resolution = dai.MonoCameraProperties.SensorResolution.THE_400_P
        left = pipeline.createMonoCamera()
        left.setBoardSocket(dai.CameraBoardSocket.LEFT)
        left.setResolution(mono_resolution)
        left.setFps(self.internal_fps)
        right = pipeline.createMonoCamera()
        right.setBoardSocket(dai.CameraBoardSocket.RIGHT)
        right.setResolution(mono_resolution)
        right.setFps(self.internal_fps)
        stereo = pipeline.createStereoDepth()
        stereo.initialConfig.setConfidenceThreshold(230)
        stereo.setLeftRightCheck(True)
        stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
        stereo.setOutputSize(left.getResolutionWidth(), right.getResolutionHeight())
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        setero_out = pipeline.createXLinkOut()
        setero_out.setStreamName("stereo_out")
        stereo.depth.link(setero_out.input)
        left.out.link(stereo.left)
        right.out.link(stereo.right)

        if self.use_hand:
            # ImageManip
            manip = pipeline.createImageManip()   
            manip.inputImage.setQueueSize(1)
            manip.inputImage.setBlocking(False)
            manip.setMaxOutputFrameSize(1228800)
            manip.initialConfig.setResize(self.pd_input_length, self.pd_input_length)

            cam.preview.link(manip.inputImage)
            manip_cfg_in = pipeline.createXLinkIn()
            manip_cfg_in.setStreamName("manip_cfg")
            manip_cfg_in.out.link(manip.inputConfig)

            # Define palm detection model
            print("Creating Palm Detection Neural Network...")
            pd_nn = pipeline.createNeuralNetwork()
            pd_nn.setBlobPath(self.pd_model)     

            # Specify that network takes latest arriving frame in non-blocking manner
            pd_nn.input.setQueueSize(1)
            pd_nn.input.setBlocking(False)
            manip.out.link(pd_nn.input)

            # Palm detection output
            pd_out = pipeline.createXLinkOut()
            pd_out.setStreamName("pd_out")
            pd_nn.out.link(pd_out.input)
            
            # Define hand landmark model
            print("Creating Hand Landmark Neural Network")  
            lm_nn = pipeline.createNeuralNetwork()
            lm_nn.setBlobPath(self.lm_model)
            lm_nn.setNumInferenceThreads(self.lm_nb_threads)

            # Hand landmark input
            self.lm_input_length = 224
            lm_in = pipeline.createXLinkIn()
            lm_in.setStreamName("lm_in")
            lm_in.out.link(lm_nn.input)

            # Hand landmark output
            lm_out = pipeline.createXLinkOut()
            lm_out.setStreamName("lm_out")
            lm_nn.out.link(lm_out.input)
        
        if self.use_yolo:
            # YoloDetectionNetwork
            print("Creating Yolo Neural Network...")
            yolo = pipeline.create(dai.node.YoloDetectionNetwork)
            yolo.setBlobPath(self.yolo_model)
            yolo.setConfidenceThreshold(self.yolo_confidence_threshold)
            yolo.input.setBlocking(False)
            yolo.setNumClasses(self.yolo_classes)
            yolo.setCoordinateSize(self.yolo_coordinates)
            yolo.setAnchors(self.yolo_anchors)
            yolo.setAnchorMasks(self.yolo_anchor_masks)
            yolo.setIouThreshold(self.yolo_iou_threshold)
            yolo_out = pipeline.create(dai.node.XLinkOut)
            yolo_out.setStreamName("yolo_out")

            # Linking nodes for the output of yolo_out
            cam.preview.link(yolo.input)            # cam.preview -> yolo.input
            yolo.out.link(yolo_out.input)           # yolo.out -> yolo_out.input

        if self.temperature_sensing:
            sysLog = pipeline.create(dai.node.SystemLogger)
            sysLog.setRate(1)  # 1 Hz
            xoutTemp = pipeline.create(dai.node.XLinkOut)
            xoutTemp.setStreamName("sysinfo")
            sysLog.out.link(xoutTemp.input)          

        print("Pipeline created.")
        return pipeline        
   

    def pd_postprocess(self, inference):
        # print(inference.getAllLayerNames())
        scores = np.array(inference.getLayerFp16("classificators"), dtype=np.float64) # 896
        bboxes = np.array(inference.getLayerFp16("regressors"), dtype=np.float16).reshape((self.nb_anchors,18)) # 896x18
        # Decode bboxes
        hands = decode_bboxes(self.pd_score_thresh, scores, bboxes, self.anchors, scale=self.pd_input_length, best_only=self.solo)
        # Non maximum suppression (not needed if solo)
        detections_to_rect(hands)
        rect_transformation(hands, self.frame_size, self.frame_size)
        return hands

    def lm_postprocess(self, hand, inference):
        hand.lm_score = inference.getLayerFp16("Identity_1")[0]  
        if hand.lm_score > self.lm_score_thresh:  
            hand.handedness = inference.getLayerFp16("Identity_2")[0]
            lm_raw = np.array(inference.getLayerFp16("Identity_dense/BiasAdd/Add")).reshape(-1,3)
            # hand.norm_landmarks contains the normalized ([0:1]) 3D coordinates of landmarks in the square rotated body bounding box
            hand.norm_landmarks = lm_raw / self.lm_input_length

            # Now calculate hand.landmarks = the landmarks in the image coordinate system (in pixel)
            src = np.array([(0, 0), (1, 0), (1, 1)], dtype=np.float32)
            dst = np.array([ (x, y) for x,y in hand.rect_points[1:]], dtype=np.float32) # hand.rect_points[0] is left bottom point and points going clockwise!
            mat = cv2.getAffineTransform(src, dst)
            lm_xy = np.expand_dims(hand.norm_landmarks[:,:2], axis=0)
            hand.landmarks = np.squeeze(cv2.transform(lm_xy, mat)).astype(np.int32)


    def next_frame(self):
        self.depth_frame = self.q_stereo_out.get().getFrame()
        in_video = self.q_video.get()
        video_frame = in_video.getCvFrame()

        if self.temperature_sensing:
            self.chipTemperature = self.qSysInfo.get().chipTemperature.average
        else:
            self.chipTemperature = None

        if self.use_yolo:
            self.yolo_detections = self.q_yolo_out.get()
            self.yolo_detections = self.yolo_detections.detections
        else:
            self.yolo_detections = []
            self.yolo_labels = []

        if not self.use_hand:
            self.hands = []
        else:
            if not self.use_previous_landmarks:
                # Send image manip config to the device
                cfg = dai.ImageManipConfig()
                # We prepare the input to the Palm detector
                cfg.setResizeThumbnail(self.pd_input_length, self.pd_input_length)
                self.q_manip_cfg.send(cfg)

            if self.pad_h:
                square_frame = cv2.copyMakeBorder(video_frame, self.pad_h, self.pad_h, self.pad_w, self.pad_w, cv2.BORDER_CONSTANT)
            else:
                square_frame = video_frame

            # Get palm detection
            if self.use_previous_landmarks:
                self.hands = self.hands_from_landmarks # Use previous landmarks
            else: # Use palm detection
                inference = self.q_pd_out.get()
                hands = self.pd_postprocess(inference)
                self.nb_frames_pd_inference += 1
                if not self.solo and self.nb_hands_in_previous_frame == 1 and len(hands) <= 1:
                    self.hands = self.hands_from_landmarks
                else:
                    self.hands = hands
        
            if len(self.hands) == 0: # No hand detected
                self.nb_frames_no_hand += 1
            
            if self.use_lm:
                nb_lm_inferences = len(self.hands)
                # Hand landmarks, send requests
                for i,h in enumerate(self.hands):
                    img_hand = warp_rect_img(h.rect_points, square_frame, self.lm_input_length, self.lm_input_length)
                    nn_data = dai.NNData()   
                    nn_data.setLayer("input_1", to_planar(img_hand, (self.lm_input_length, self.lm_input_length)))
                    self.q_lm_in.send(nn_data)
                
                # Get inference results
                for i,h in enumerate(self.hands):
                    inference = self.q_lm_out.get()
                    #if i == 0: self.glob_lm_rtrip_time += now() - lm_rtrip_time
                    self.lm_postprocess(h, inference)

                # Filter hands with low confidence
                self.hands = [ h for h in self.hands if h.lm_score > self.lm_score_thresh]

                self.hands_from_landmarks = [hand_landmarks_to_rect(hand) for hand in self.hands]
                
                nb_hands = len(self.hands)

                # Stats
                if nb_lm_inferences: self.nb_frames_lm_inference += 1
                self.nb_lm_inferences += nb_lm_inferences
                self.nb_failed_lm_inferences += nb_lm_inferences - nb_hands 
                if self.use_previous_landmarks: self.nb_frames_lm_inference_after_landmarks_ROI += 1

                self.use_previous_landmarks = True
                if nb_hands == 0:
                    self.use_previous_landmarks = False
                #else: cv2.imshow("img_hand", img_hand)
                
                self.nb_hands_in_previous_frame = nb_hands           
                
                for hand in self.hands:
                    # If we added padding to make the image square, we need to remove this padding from landmark coordinates and from rect_points
                    if self.pad_h > 0:
                        hand.landmarks[:,1] -= self.pad_h
                        for i in range(len(hand.rect_points)):
                            hand.rect_points[i][1] -= self.pad_h
                    if self.pad_w > 0:
                        hand.landmarks[:,0] -= self.pad_w
                        for i in range(len(hand.rect_points)):
                            hand.rect_points[i][0] -= self.pad_w

                    # Set the hand label
                    hand.label = "right" if hand.handedness > 0.5 else "left"
        
        return video_frame, self.hands, self.yolo_detections, self.yolo_labels, self.img_h, self.img_w, self.depth_frame, self.chipTemperature


    def exit(self):
        self.device.close()       


###################### POGRAMA PRINCIPAL ######################
import cv2, serial, time, math, os, subprocess
import scipy.io as sio
import numpy as np

# Coordenadas de los vertices un bounding box
def Vertices(detection):        
    x1 = int(detection.xmin * width)
    x2 = int(detection.xmax * width)
    y1 = int(detection.ymin * height)
    y2 = int(detection.ymax * height)
    return x1, x2, y1, y2

# Coordenada del centro de un bounding box
def Center(x1, x2, y1, y2):
    x = int((x1 + x2) / 2)
    y = int((y1 + y2) / 2)
    return x, y

# Coordenadas del centro del bounding box mÃ¡s cercano a un punto de referencia
def Nearest_Coordinate(refPoint, centroids):
    x_ref, y_ref = refPoint
    minDist = min((x-x_ref)**2 + (y-y_ref)**2 for x, y in centroids)
    for index, (x, y) in enumerate(centroids):
        if (x-x_ref)**2 + (y-y_ref)**2 == minDist:
            return x, y, index

# FunciÃ³n que crea una ROI a partir de una coordenada y un radio DELTA
def ROI(COORDINATE, DELTA):
    x, y = COORDINATE
    x1 = x - DELTA
    x2 = x + DELTA
    y1 = y - DELTA
    y2 = y + DELTA
    return x1, x2, y1, y2

# FunciÃ³n mapeada las coordenadas de una ROI x1, x2, y1, y2 en coordenadas de profundidad
def ROI2DepthROI(COORDINATES):
    x1, x2, y1, y2 = COORDINATES
    x1_depth = int((x1+192*2)/1024*640) - 120
    x2_depth = int((x2+192*2)/1024*640) - 120
    y1_depth = int(y1/1024*640)
    y2_depth = int(y2/1024*640)
    return x1_depth, x2_depth, y1_depth, y2_depth

# FunciÃ³n que calcula el promedio de los valores de profundidad de un ROI en metros
def AverageDepth(ROI, depthFrame):
    x1, x2, y1, y2 = ROI
    depth = depthFrame[y1:y2, x1:x2]
    z = np.nanmean(depth)/1000
    return z

# funciones anÃ³nimas para incremento de la frecuencia de pulsos en los vibradores
f1 = lambda x: math.sqrt(1 + x) - 1
f2 = lambda x: (x + 1)**2 - 1

##################### CONSTANTES Y CONFIGURACIONES #####################
width = height = 640 # ResoluciÃ³n de entrada de la red neuronal
VideoRGB = cv2.VideoWriter('VideoRGB.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))

# RegiÃ³n de interÃ©s (ROI) para la estimaciÃ³n de la distancia de obstÃ¡culos z
DELTA = 30 # Radio de la ROI Cental en pixeles
DELTA_DOLL = 15 # Umbral de distancia en pixeles entre la referencia y el centroide de la detecciÃ³n mÃ¡s cercana (centroid of the nearest detection CND)
x_center, y_center = width//2, height//2 # Coordenadas del centro de la imagen
CentralROI = ROI((x_center, y_center), DELTA)

# ConfiguraciÃ³n del buzzer
bps = 10 # bits por segundo (envÃ­o de datos para sonar el buzzer por segundo)
zmin, zmax = 1, 2 # Umbral de distancia mÃ­nima y mÃ¡xima para sonar el buzzer

# Estilos de dibujo (colores y timpografÃ­a)
BoxesColor = (0, 255, 0)
BoxesSize = 2
LineColor = (0, 0, 255)
CircleColor = (255, 0, 0)
TextColor = (0,0,255)
FontFace = cv2.FONT_HERSHEY_SIMPLEX
FontSize = 1

voice_messages = [
    'sÃ­mbolo aviÃ³n', 'recogida de equipajes', 'baÃ±os', 'peligro electricidad', 'flecha hacia abajo',
    'flecha hacia abajo de emergencia', 'salida de emergencia', "flecha izquierda de emergencia",
    "flecha derecha de emergencia", "flecha arriba de emergencia", "sÃ­mbolo de extintor", "extintor", 
    "preferencial", "flecha izquierda", "no pasar", "restaurantes", "flecha derecha", "flecha izquierda",
    "flecha derecha", "flecha arriba"]

##################### inicializaciÃ³n de variables #####################
times = []      # Muestras de los tiempos de ejecuciÃ³n del programa desde la primera captura de frame
d = []   # Muestras de la distancias a la detecion mÃ¡s cercana con visiÃ³n monocular
z = []   # Muestras de las distancias a los obstÃ¡culos en la ROI central con visiÃ³n estereoscÃ³pica
h = []   # Muestras de las distancias horizontales en piexeles de la detecciÃ³n mÃ¡s cercana al centro de la imagen
v = []   # Muestras de las distancias verticales en piexeles de la detecciÃ³n mÃ¡s cercana al centro de la imagen 

haptic_messages  = []  # Muestras de los mensajes de activaciÃ³n del vibrador UP de la interfaz hÃ¡ptica
buzzer_messages  = []  # Muestras de los mensajes de activaciÃ³n del buzzer
nearest_labels = []  # Muestras de la etiquta de la deteciÃ³n mÃ¡s cercana

# Muestras de temperatura
chipTemperatures = []  # Muestras de la temperatura del chip
cpuTemperature = []  # Muestras de la temperatura del CPU
max6675Temperature = []  # Muestras de la temperatura del sensor DHT22

# DeclaraciÃ³n de variables
mentioned_object = False # bandera booleana para evitar que se repita el nombre del objeto detectado
mentioned_hand_use = False # bandera booleana para evitar que se repita el la mencion TTS del uso de la mano
frame_start_time = 0
frames_timer = 0
frames_counter = 0
start_move_time = 0
start_buzze_time = 0

# Rutas de los modelos neuronales y configuraciones
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PALM_DETECTION_MODEL = str(SCRIPT_DIR / "Models/Hands Models/palm_detection_sh4.blob")
LANDMARK_MODEL_LITE = str(SCRIPT_DIR / "Models/Hands Models/hand_landmark_lite_sh4.blob")
MY_YOLO_MODEL = str(SCRIPT_DIR / "Models/BVI Models/best3_openvino_2021.4_6shave.blob")
YOLO_CONFIG = str(SCRIPT_DIR / "Models/BVI Models/best3.json")
os.chdir(SCRIPT_DIR)# Cambiar la ruta de ejecuciÃ³n al directorio del script actual

##################### InicializaciÃ³n de objetos #####################
# Detectar si el sistema operativo es Raspbian para usar la termocupla MAX6675
try:
    import RPi.GPIO, max6675
    # set the pin for communicate with MAX6675
    cs = 22
    sck = 18
    so = 16
    max6675.set_pin(cs, sck, so, 1)
    Measure = True
    print("temperature sensing is enable")
except(ImportError, RuntimeError):
    Measure = False
    print("temperature sensing is disable")

# Inicializar el objeto para la detecciÃ³n de manos y seÃ±alizaciones de espacios interiores
tracker = YoloHandTracker(
    temperature_sensing = Measure,
    use_hand = False,
    use_yolo = False,
    yolo_model = MY_YOLO_MODEL,
    yolo_configurations = YOLO_CONFIG,
    )

visualize = True
try:# Intentar establecer un objeto para comunicaciÃ³n serial a usando UART 
    serial = serial.Serial("/dev/ttyS0", 9600, timeout=1)
    serial_is_connected = True
except:
    serial_is_connected = False



##################### Bucle principal #####################
loop_start_time = time.time()
while True:
    frame, hands, yolo_detections, labels, width, height, depthFrame, chip_temperature = tracker.next_frame()

    # Almacenar la distancia de los obstÃ¡culos en la ROI central
    z.append(AverageDepth(CentralROI, depthFrame))

    # DedecciÃ³n de obstÃ¡culos en la ROI Central
    """ Uso del buzzer para sonar mÃ¡s frecuentemente a medida que se acerca un objeto en la ROI central"""
    if z[-1] < zmax and z[-1] > zmin: # si el obstÃ¡culo estÃ¡ a una distancia menor a zmax y mayor a zmin
        # Calcular el tiempo de espera entre cada envÃ­o de datos para sonar el buzzer
        wait_time_buzzer = 1/(bps)*z[-1]/(zmax-zmin) # segundos
        if f1(time.time() - start_buzze_time) > f2(wait_time_buzzer):
            if serial_is_connected: serial.write(b'0') # Sonar el buzzer
            start_buzze_time = time.time()
            buzzer_messages.append('1')
        else:
            buzzer_messages.append('nan')
    else:
        buzzer_messages.append('nan')

    # DetecciÃ³n de la mano del usuario
    """ Si se detecta una mano, se determina si la distandia de la muÃ±eca a la cÃ¡mera es menor a 1 metro,
    si es asÃ­, se le informa al usuario que que su mano estÃ¡ siendo untilizada como referencia para la 
    detecciÃ³n del objeto mÃ¡s cercano, de lo contrario se usa centro de la imagen como referencia """

    x_ref, y_ref = (x_center, y_center) # Usar coordenadas del centro de la imagen como referencia en primera instancia
    if len(hands) > 0: # Si se detecta la mano del usuario, cambiar la referencia a la punta del dedo Ã­ndice
        x_doll, y_doll = hands[0].landmarks[0,:2] # Coordenadas de la muÃ±eca
        x_index_finger, y_index_finger = hands[0].landmarks[8,:2] # Coordenadas de la punta del dedo Ã­ndice
        dollROI = ROI((x_doll, y_doll), DELTA_DOLL) # ROI de la muÃ±eca en la imagen de color
        dollDepthROI = ROI2DepthROI(dollROI) # ROI de la muÃ±eca en la imagen de profundidad
        dollDistance = AverageDepth(dollDepthROI, depthFrame) # Distancia de la muÃ±eca a la cÃ¡mara
        color_use_hand = (0, 0, 255) # Color rojo si la punta del dedo Ã­ndice estÃ¡ siendo utilizada como referencia
        
        # Identificar si es la mano del usuario o no en base a la distancia de la muÃ±eca a la cÃ¡mara
        if np.isnan(dollDistance) or dollDistance < .5: # Si la distancia de la muÃ±eca a la cÃ¡mara es menor a 50 cm
            x_ref, y_ref = x_index_finger, y_index_finger # Usar coordenadas de la punta del dedo Ã­ndice como referencia
            color_use_hand = (0, 255, 0) # Color verde si la punta del dedo Ã­ndice estÃ¡ siendo utilizada como referencia

            if not mentioned_hand_use:
                os.system("spd-say 'Dedo Ã­ndice como referencia'") # Informar al usuario que su mano estÃ¡ siendo utilizada como referencia
                mentioned_hand_use = True # Activar la bandera para evitar que se repita el mensaje de uso de la mano
        else:
            mentioned_hand_use = False # Desactivar la bandera para que se vuelva a mencionar el uso de la mano

        if visualize: # Visualizar en rojo la ROI de la muÃ±eca y la punta del dedo Ã­ndice
            cv2.putText(frame, f"{dollDistance:.2f} m", (dollROI[1], dollROI[2]+DELTA_DOLL), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_use_hand, 1, cv2.LINE_AA)
            cv2.rectangle(frame, (dollROI[0], dollROI[2]), (dollROI[1], dollROI[3]), color_use_hand, 1)
            cv2.circle(frame, (x_doll, y_doll), 5, color_use_hand, -1) # Dibujar un cÃ­rculo rojo en la muÃ±eca
            cv2.circle(frame, (x_index_finger, y_index_finger), 5, color_use_hand, -1) # Dibujar un cÃ­rculo rojo en la punta del dedo Ã­ndice
            cv2.rectangle(depthFrame, (dollDepthROI[0], dollDepthROI[2]), (dollDepthROI[1], dollDepthROI[3]), color_use_hand, 1)

    # DetecciÃ³n del centro del bounding box mÃ¡s cercano al punto de referencia (CNBB)
    if len(yolo_detections) > 0:
        Centroids = []  # Coordenadas del centro de los objetos detectados
        for detection in yolo_detections:
            x1, x2, y1, y2 = Vertices(detection)
            x, y = Center(x1, x2, y1, y2)
            Centroids.append((x, y))
            label = labels[detection.label]
            confidence = detection.confidence * 100
            distance = AverageDepth(ROI2DepthROI(Vertices(detection)), depthFrame)
            if visualize:
                cv2.putText(frame, label , (x1, y1), FontFace, FontSize, TextColor, 2)
                cv2.putText(frame, "{:.0f} %".format(confidence), (x2, y), FontFace, FontSize, TextColor, 1)
                cv2.putText(frame, "{:.2f} [m]".format(distance) , (x2, y2), FontFace, FontSize, TextColor)
                cv2.rectangle(frame, (x1, y1), (x2, y2), BoxesColor, BoxesSize) 
        x, y, index = Nearest_Coordinate((x_ref, y_ref), Centroids) # Coordenadas del CNBB
        nearest_labels.append(labels[yolo_detections[index].label]) # Almacenar la etiqueta del CNBB
        d.append(AverageDepth(Vertices(yolo_detections[index]), depthFrame)) # Almacenar la profundidad del CNBB
        if visualize: cv2.arrowedLine(frame, (x_ref, y_ref), (x, y), LineColor, 2) # Dibujar una flecha desde el punto de referencia al CNBB

        # ComunicaciÃ³n serial para actuadores vibrotÃ¡ctiles y buzzer
        """ EnvÃ­o de mensajes para indicarcaciones vibrotÃ¡ctiles {up,down,left,right} al usuario,
        y mencionar el nombre del objeto detectado mÃ¡s cercano si se entra en la ROI central
        mediante una voz sintetizada en el microprocesador. """
        h.append( abs(x - x_ref) )
        v.append( abs(y - y_ref) )
        if h[-1] < DELTA and v[-1] < DELTA:
            """ Decir mensaje traducido al espaÃ±ol, basado en el objeto detectado mÃ¡s cercano y 
            la distancia a la cÃ¡mara a la cual se encuentra. """
            if not mentioned_object:
                haptic_messages.append('c')
                msg = voice_messages[yolo_detections[index].label]
                os.system("spd-say '" + msg + "{:.2f} [m]".format(d[-1]) + " metros'")
                if serial_is_connected: serial.write(b'0')
                mentioned_object = True
            else:
                haptic_messages.append('nan')
        else:
            if h[-1] > v[-1]:
                wait_time = 1/bps * h[-1]/((width-DELTA)//2)
                if f1(time.time() - start_move_time) > f2(wait_time):
                    if (x - x_ref) > 0: # El objeto estÃ¡ a la derecha del centro de la imagen
                        if serial_is_connected: serial.write(b'r') # 68 ASCII
                        haptic_messages.append('r')
                    else: # El objeto estÃ¡ a la izquierda del centro de la imagen
                        if serial_is_connected: serial.write(b'l') # 76 ASCII
                        haptic_messages.append('l')
                    start_move_time = time.time()
                else:
                    haptic_messages.append('nan')
            else:
                wait_time = 1/bps * v[-1]/((height-DELTA)//2)
                if f1(time.time() - start_move_time) > f2(wait_time):
                    if (y - y_ref) > 0: # El objeto estÃ¡ abajo del centro de la imagen
                        if serial_is_connected: serial.write(b'd') # 82 ASCII
                        haptic_messages.append('d')
                    else: # El objeto estÃ¡ arriba del centro de la imagen
                        if serial_is_connected: serial.write(b'u') # 85 ASCII
                        haptic_messages.append('u')
                    start_move_time = time.time()
                else:
                    haptic_messages.append('nan')
            mentioned_object = False
    else:
        haptic_messages.append("nan")
        nearest_labels.append("nan")
        d.append("nan")
        h.append("nan")
        v.append("nan")

    # Contador de FPS 
    frames_counter += 1
    current_time = time.time()
    times.append(current_time - loop_start_time) # Almacenar el tiempo de ejecuciÃ³n
    if (current_time - frames_timer) > 1:
        if Measure: # Almacenar la temperatura del chip de sensor OAK-D cada segundo
            chipTemperatures.append(chip_temperature)
            max6675Temperature.append(max6675.read_temp(cs)) # Temperatura del sensor DHT22
            cpuTemperature.append(float(subprocess.check_output("vcgencmd measure_temp", shell=True).decode("utf-8").replace("temp=","").replace("'C\n",""))) # Temperatura de la CPU de la Raspberry Pi
        fps = ( frames_counter / (current_time - frames_timer) )
        frames_counter = 0
        frames_timer = current_time

    if visualize:
        # Visualizar la mapa de profundidad
        depthFrameColor = cv2.normalize(depthFrame, None, 255, 0, cv2.NORM_INF, cv2.CV_8UC1)
        depthFrameColor = cv2.equalizeHist(depthFrameColor)
        depthFrameColor = cv2.applyColorMap(depthFrameColor, cv2.COLORMAP_JET)
        cv2.imshow("depth", depthFrameColor)

        # Mostrar el frame CentralROI y la profundidad de los obstÃ¡culos en la ROI central
        cv2.rectangle(frame, (CentralROI[0], CentralROI[2]), (CentralROI[1], CentralROI[3]), LineColor, 2)
        cv2.putText(frame, "{:.2f} m".format(z[-1]), (CentralROI[1], CentralROI[2]+DELTA), FontFace, FontSize, TextColor, 1)
        # Mostrar FPS y tiempo de ejecuciÃ³n
        cv2.putText(frame, "fps: {:.2f}".format(fps), (0,height-FontSize-6), FontFace, FontSize, TextColor, 2)
        cv2.putText(frame, "t: " + ("{:.2f} s".format(times[-1])), (0, 25), FontFace, FontSize, TextColor, 2) 
        # Mostrar el frame de la cÃ¡mara RGB
        cv2.imshow("frame", frame)

    # Salir del programa si alguna de estas teclas son presionadas {ESC, SPACE, q}
    if cv2.waitKey(1) in [27, 32, ord('q')]:
        break

tracker.exit()

# Guardar los datos en un archivo .mat
if Measure: 
    sio.savemat('data.mat', {
        'z': z,
        'd': d,
        'h': h,
        'v': v,
        'times': times,
        'haptic_messages': haptic_messages,
        'nearest_labels': nearest_labels,
        'chipTemperatures': chipTemperatures,
        'max6675Temperature': max6675Temperature,
        'cpuTemperature': cpuTemperature
    })  
